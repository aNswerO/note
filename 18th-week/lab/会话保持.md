# HTTP的无状态：
>HTTP是无状态协议，即对事务处理没有记忆能力，服务端无法得知2次请求之间的联系
+ 两种用于保持HTTP连接状态的技术：
    + cookie：
        ```
        存放在客户端，通过客户端保持状态；cookie由服务端发送给客户端，并以文本形式存放于客户端，客户端再次向服务器发送请求时都会带上cookie（存放于HTTP Request Header）
        ```

    + session：
        ```
        存放在服务端内存中（若未进行持久化则易丢失），通过服务端保持状态；客户端第一次向服务端发起HTTP请求时，服务端会生成一个随机值即SessionID并将其发送给客户端，客户端在发生之后的请求时会带上这个SessionID，服务端通过查找这个SessionID是否存在来判断此次请求的客户端之前是否来过；Session会定时过期，客户端带着过期的SessionID访问，服务端查找不到这个过期的SessionID（因为过期被清除）就会发送一个新的SessionID
        ```
# HTTP的有连接：
>HTTP基于TCP协议，是面向连接的，需要3次握手、4次断开
# HTTP的短连接和长连接：
+ 短连接：HTTP1.1之前，都是一个请求一个连接，然而TCP的连接创建销毁成本高，对服务器影响很大

+ 长连接：HTTP1.1之后，支持了keep-alive，即长连接，一个连接打开后，会保持一段时间，客户端再次向服务端发起请求时就使用这个保持的TCP连接，减轻服务端压力的同时提高了效率
# session保持的方式：
## 规划：
|主机名|主机角色|IP|服务|
|--|--|--|--|
|lb.qyh.com|负载均衡器|192.168.6.10|nginx、httpd|
|backend1.qyh.com|后端web服务器|192.168.6.20|JDK8、tomcat8|
|backend2.qyh.com|后端web服务器|192.168.6.21|JDK8、tomcat8|
## session sticky（会话粘滞）：
### 使用nginx调度：
+ 每台主机做本地域名解析：
    ```
    192.168.6.10 lb.qyh.com
    192.168.6.20 backend1.qyh.com
    192.168.6.21 backend2.qyh.com
    ```
+ 安装及环境变量配置略过
+ 在两台后端web服务器上创建虚拟主机的目录：
    + 创建目录：
        ```
        mkdir /data/webapps/ROOT
        ```
    + 创建测试页面：
        ```
        vim /data/webapps/ROOT/index.jsp

        <%@ page import="java.util.*" %>
        <!DOCTYPE html>
        <html lang="en">
        <head>
        <meta charset="UTF-8">
        <title>lbjsptest</title>
        </head>
        <body>
        <div>On <%=request.getServerName() %></div>
        <div><%=request.getLocalAddr() + ":" + request.getLocalPort() %></div>
        <div>SessionID = <span style="color:blue"><%=session.getId() %></span></div>
        <%=new Date()%>
        </body>
        </html>
        ```
+ 编辑web服务器的tomcat配置文件：
    ```
    vim /usr/lcoal/tomcat/conf/server.xml
    ```
    >修改Engine中的defaulthost，并在Engine下添加一个新的host
    + backend1主机：
        ```conf
        <Engine name="Catalina" defaultHost="backend1.qyh.com">
        <Host name="backend1.qyh.com" appBase="/data/webapps/" unpack="true" autoDeploy="true">
        ```
    + backend2主机：
        ```conf
        <Engine name="Catalina" defaultHost="backend2.qyh.com">
        <Host name="backend2.qyh.com" appBase="/data/webapps/" unpack="true" autoDeploy="true">
        ```
+ 编辑负载均衡器的nginx配置文件：
    ```
    vim /etc/nginx/nginx.conf
    ```
    >在http配置段下添加upstream配置，并在server配置段下添加一个location配置
    ```conf
    upstream tomcats {
    #   ip_hash
        server backend1.qyh.com:8080;
        server backend2.qyh.com:8080;
    }

    location ~* \.(jsp|do)$ {
            proxy_pass http://tomcats;
        }
    ```
+ 测试：
1. 先注释负载均衡器的nginx配置文件中的ip_hash，查看轮询调度下的sessionID情况：  
![avagar]()  
![avagar]()  
![avagar]()  
![avagar]()  
>可以看到sessionID随着轮询调度一直在发生变化，哪怕被调度到同一台后端web服务器，sessionID也在变化；  
这是因为当客户端访问过backend1，得到了backend1给它的sessionID，下一次访问时，因轮询调度，它拿着backend1的sessionID去找backend2，backend2不认这个sessionID，所以给客户端发了一个新的，当下一次访问时，，客户端因轮询调度再次访问到backend1，客户端又拿着这个backend2给它的sessionID去找backend1，这回backend1也不认了，所以又发给客户端一个新的sessionID，所以sessionID是每次刷新页面都会变化的，根本无法保持会话
2. 将ip_hash的注释取消，查看源地址哈希调度下的sessionID情况：  
+ 两台后端服务器都正常：
![avagar]()  
![avagar]()  
![avagar]()  
>可以发现由于ip_hash调度算法，客户端一直访问到同一台后端服务器，所以sessionID也保持不变
+ 有一台出现故障：
![avagar]()  
>虽然这种方法可以做到session保持，但当有后端服务器出现故障，并且正好是用户之前访问的那台，那么由于这台服务器故障，用户的请求全被转发到其他服务器，session信息自然也就发生变化了，在这种情况下还是无法做到会话保持，因为哪怕故障服务器恢复上线，用户拿着其他服务器给的sessionID去访问这个服务器，这个服务器也不认了
### 使用httpd调度：
+ 将后端tomcat服务器配置文件中的Engine中添加属性：
    >这样可以使测试页面中的sessionID中多出一个“tomcatX”后缀
    + backend1：
        ```
        vim /usr/local/tomcat/conf/server.xml

        <Engine name="Catalina" defaultHost="backend1.qyh.com" jvmRoute="tomcat1">
        ```
    + backend2：
        ```
        vim /usr/local/tomcat/conf/server.xml

        <Engine name="Catalina" defaultHost="backend2.qyh.com" jvmRoute="tomcat2">
        ```
+ 编辑负载均衡器的httpd的配置文件，取消默认主机：
    >将DocumentRoot"/var/www/html"一行注释
    ```
    vim /etc/httpd/conf/httpd.conf
    ```
+ 创建一个虚拟主机配置文件，内容如下：
    ```sh
    vim /etc/httpd/conf.d/vhost.conf

    Header add Set-Cookie "ROUTEID=.%{BALANCER_WORKER_ROUTE}e; path=/" env=BALANCER_ROUTE_CHANGED
    <VirtualHost *:80>
    ServerName lb.qyh.com
    ProxyRequests Off
    ProxyVia On
    ProxyPreserveHost On
    ProxyPass / balancer://lbtomcats/
    ProxyPassReverse / balancer://lbtomcats/
    </VirtualHost>
    <Proxy balancer://lbtomcats>
    BalancerMember http://backend1.qyh.com:8080 loadfactor=1 route=Tomcat1
    #loadfactor定义了后端服务器的权重，取值范围1~100
    BalancerMember http://backend2.qyh.com:8080 loadfactor=1 route=Tomcat2
    ProxySet stickysession=ROUTEID
    </Proxy>
    ```
+ 检查语法、启动httpd服务并测试：
    + 检查语法：
        ```
            httpd -t
        ```
    + 启动httpd：
        ```
            systemctl start httpd
        ```
    + 测试：  
    ![avagar]()  
    ![avagar]()  
    >可以看到请求一直被调度到同一个后端服务器，所以session也得到了保持，但同上面nginx调度一样，这种方法并不能做到真正的会话保持
## tomcat session cluster：
+ 配置说明：
    + Cluster：集群配置

    + Manager：会话管理配置
    + Channel：信道配置
        + Membership：成员判定
            ```
            使用什么多播地址、端口多少、间隔时长（ms）、超市市场（ms）  
            同一个多播地址和同一个端口被认为是同属一组
            ```
            >组播地址范围为224.0.0.0 ~ 239.255.255.255
        + Receive：接收器
            ```
            多线程接收多个其他节点的心跳、会话信息
            默认会从4000到4100依次尝试可用端口
            ```
            >address=“auto”，若设定为auto，则可能会绑定到127.0.0.1上，所以要配置成可用的IP
        + sender：多线程发送器，内部使用了tcp连接池
        + interceptor：拦截器
    + value：
        + ReplicationValue：
            ```
            检测哪些请求需要检测session，session数据是否有了变化，需要启动复制过程
            ```
    + ClusterListener：
        + ClusterSessionListener：集群session侦听器
+ 编辑两台后端服务器的server.xml文件：
    + backend1：
        ```conf
        <Cluster className="org.apache.catalina.ha.tcp.SimpleTcpCluster"
        channelSendOptions="8">
            <Manager className="org.apache.catalina.ha.session.DeltaManager"
            expireSessionsOnShutdown="false"
            notifyListenersOnReplication="true"/>
            <Channel className="org.apache.catalina.tribes.group.GroupChannel">
                <Membership className="org.apache.catalina.tribes.membership.McastService"
                address="230.16.166.6"
                port="44664"
                frequency="500"
                dropTime="3000"/>
                <Receiver className="org.apache.catalina.tribes.transport.nio.NioReceiver"
                address="192.168.6.20"
                port="4000"
                autoBind="100"
                selectorTimeout="5000"
                maxThreads="6"/>
                <Sender className="org.apache.catalina.tribes.transport.ReplicationTransmitter">
                    <Transport className="org.apache.catalina.tribes.transport.nio.PooledParallelSender"/>
                </Sender>
                <Interceptor className="org.apache.catalina.tribes.group.interceptors.TcpFailureDetector"/>
                <Interceptor className="org.apache.catalina.tribes.group.interceptors.MessageDispatchInterceptor"/>
            </Channel>
            <Valve className="org.apache.catalina.ha.tcp.ReplicationValve"
            filter=""/>
            <Valve className="org.apache.catalina.ha.session.JvmRouteBinderValve"/>
            <Deployer className="org.apache.catalina.ha.deploy.FarmWarDeployer"
            tempDir="/tmp/war-temp/"
            deployDir="/tmp/war-deploy/"
            watchDir="/tmp/war-listen/"
            watchEnabled="false"/>
            <ClusterListener className="org.apache.catalina.ha.session.ClusterSessionListener"/>
        </Cluster>
        ```
    + backend2：
        >只需要将Receiver下的address="192.168.6.20"改为address="192.168.6.21"，其余内容与backend1一样
+ 在两台后端服务器创建目录，并复制一份web.xml放进去：
    >不进行这步配置，session无法保持
    ```
        mkdir /data/webapps/ROOT/WEB-INF/

        cp /usr/local/tomcat/conf/web.xml /data/webapps/ROOT/WEB-INF/
    ```
+ 重启两台后端服务器的tomcat：
    ```
        catalina.sh stop

        catalina.sh start
    ```
+ 测试：  
![avagar]()  
![avagar]()  
![avagar]()  
>第三张图为模拟192.168.6.21服务器故障下线，可以看到访问到另一台服务器，且sessionID也没有发生变化，实现了session保持